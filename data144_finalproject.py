# -*- coding: utf-8 -*-
"""Data144_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EG1FoOHKYmu0Q5WYMwNvkaAdg_Wp6dJI
"""

#SF Airbnb Open Data
NAME = "Sneha Somaya, Yin Yin Teo, Erin Liu, Chandrima Sabharwal"

"""**GOAL:** 

- Cluster AirBnB's in San Francisco based on quantitative and qualitative features with a geographical visualization to understand what kind of Airbnbs are similar

### **1. Setting up imports and data**
"""

# Commented out IPython magic to ensure Python compatibility.
#imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import re
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize

# %matplotlib inline

#listings = pd.read_csv('/content/listings.csv.gz',engine='python',encoding = 'utf-8',error_bad_lines=False)
listings=pd.read_csv('listings.csv')
listings.head()

#reviews = pd.read_csv('/content/reviews.csv.gz',engine='python',encoding = 'utf-8',error_bad_lines=False)
reviews=pd.read_csv('reviews.csv',engine='python',encoding = 'utf-8',error_bad_lines=False)
reviews.head()

joined = pd.merge(listings, reviews, how='inner', left_on='id', right_on='listing_id')
joined.head()

df_dropped = joined.drop(columns = ['listing_url', 'scrape_id', 'last_scraped', 'id_x','host_url', 'host_thumbnail_url', 'neighbourhood_group_cleansed', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights','maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated', 'license', 'id_y'])
df_dropped.head()

listing_comments = df_dropped[{'listing_id', 'comments'}]
listing_comments.comments = listing_comments.comments.astype(str)
grouped_comments = listing_comments.groupby(['listing_id'])['comments'].apply(lambda x: ' '.join(x)).to_frame()
grouped_comments.head()

#numerical 
listings2 = listings.copy()

listing2num = listings2[["accommodates", "beds", "bedrooms", "price", "minimum_nights", "maximum_nights", "number_of_reviews", "review_scores_rating", "bathrooms_text", "id"]]
listing2num['price'] = listing2num['price'].str[1:]
listing2num['price'] = listing2num.iloc[:,3].str.replace(',', '').astype(float)

listing2num['bathrooms_text'] = listing2num.bathrooms_text.str.extract('(\d+)').astype(float)

listing2num = listing2num.rename(columns={'bathrooms_text': 'bathrooms'})
listing2num.dropna(inplace=True)
listing2num

#categorical
listings2cat = listings2[["neighbourhood_cleansed", "property_type", "room_type", "bathrooms_text"]]
listings2cat

listings2cat.neighbourhood_cleansed.value_counts()

listings2nume = listing2num.copy()
#remove outliers
listings2nume = listings2nume[listings2nume.maximum_nights < 2000]
listings2nume = listings2nume[listings2nume.price < 9999]

listings2nume['property_type'] = listings2['property_type']
listings2nume['property_type'].unique() #replaced with private, entire space and shared space
def check_property_type(x):
  if "Private" in x:
    return "Private Space"
  elif "Shared" in x:
    return "Shared Space"
  else:
    return "Entire Space"
listings2nume.property_type = listings2nume.property_type.apply(check_property_type)
one_hot_property_type = pd.get_dummies(listings2nume.property_type)
listings_joined = listings2nume.join(one_hot_property_type)
listings_joined_dropped = listings_joined.drop(columns=['property_type'])
listings_joined_dropped

"""## **2. EDA - Correlation Matrices & Other Visualisations** """

#creating correlation matrix: numerical values
plt.figure(figsize=(20,10))
cor = listing2num.corr()

sns.heatmap(cor, annot=True, cmap=plt.cm.Blues)
plt.show()

#print out top 7 highest correlations for numerical features
print(listing2num[["accommodates", "beds"]].corr())
print(listing2num[["accommodates", "bedrooms"]].corr())
print(listing2num[["beds", "bedrooms"]].corr())
print(listing2num[["accommodates", "bathrooms"]].corr())
print(listing2num[["beds", "bathrooms"]].corr())
print(listing2num[["bedrooms", "bathrooms"]].corr())
print(listing2num[["minimum_nights", "maximum_nights"]].corr())

#other visualisations for numerical
import requests
import bs4 as bs
import pandas as pd
import numpy as np
import sklearn as sl
import sklearn.preprocessing as preprocessing
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

plt.hist(listing2num['accommodates'], bins = 24, color='r', label='Accommodates',alpha=0.8)
plt.title('Frequency of Accommodates')
plt.legend(loc='upper right')
plt.xlabel('Accommodates')
plt.ylabel('frequency')

plt.xticks(rotation = 45)
plt.show()

sns.histplot(listing2num.accommodates)

plt.hist(listing2num['beds'], bins = 24, color='b', label='Beds',alpha=0.8)
plt.title('Frequency of Beds')
plt.legend(loc='upper right')

plt.hist(listing2num['bedrooms'], bins = 24, color='y', label='Bedrooms',alpha=0.8)
plt.title('Frequency of Bedrooms')
plt.legend(loc='upper right')

plt.show()

sns.set(rc={'figure.figsize':(20,5)})
review_vs_price = sns.stripplot(x='review_scores_rating', y='price', data = listing2num, jitter=True)
review_vs_price.set(ylim=(0, 3500))
plt.show()

sns.set(rc={'figure.figsize':(10,5)})
sns.stripplot(x='accommodates', y='review_scores_rating', data = listing2num, jitter=True)
plt.show()

sns.set(rc={'figure.figsize':(10,5)})
sns.stripplot(x='bedrooms', y='review_scores_rating', data = listing2num, jitter=True)
plt.show()

"""**Visualizing with Categorical Features**"""

#neighbourhood_cleansed & room_type
listings2cat_2 = listings2cat[['neighbourhood_cleansed', "room_type"]]
listings2cat_d2 = pd.get_dummies(listings2cat_2) 
listings2cat_d2

sns.catplot(x="room_type", y="neighbourhood_cleansed", jitter=False, height=10, data=listings2cat_2)

sns.histplot(listings_joined['property_type'])

x = [listing2num['beds'],listing2num['bedrooms'],listing2num['accommodates']]
plt.hist(x,color=['lightblue','tan','lightcoral']) 
plt.legend(["Beds","Bedrooms", "Accommodates"])
plt.title('Frequency')
plt.xlabel('Count')
plt.show()
plt.savefig(r'1.png')

"""## **3. TFIDF**

**We used TF-IDF to quantify each hosts description of their AirBnB in order to add a weight to each posting which can help us cluster better. TF-IDF is useful as it is a relative metric comparing across text only in our data.**
"""

# "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. ... 
# It has many uses, most importantly in automated text analysis, and 
# is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP)."

# source: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76

!python -m nltk.downloader stopwords

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

#cleaning the descriptions
from nltk.corpus import stopwords

def clean_text(string_in):
  string_in = re.sub("[^a-zA-Z]", " ", str(string_in))  # Replace all non-letters with spaces
  string_in = string_in.lower()                         # Tranform to lower case    
  return string_in.strip()

listings2nume["descriptions"] = listings2.name.apply(clean_text)

#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
data = listings2nume["descriptions"]

tf_idf_vectorizor = TfidfVectorizer(stop_words = 'english',#tokenizer = tokenize_and_stem,
                             max_features = 20000)
tf_idf = tf_idf_vectorizor.fit_transform(data)
tf_idf_norm = normalize(tf_idf)
tf_idf_array = tf_idf_norm.toarray()

scores = pd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names())
scores = np.mean(scores).sort_values(ascending=False).to_frame('Score').reset_index().rename(columns={'index':'Word'})
scores_dict = dict(zip(scores['Word'], scores['Score']))
listings2nume['TF_IDF_Weight'] = 0

desc = listings2nume['descriptions'].reset_index().drop('index',axis=1)
desc = [x.split() for x in listings2nume["descriptions"]]

scoreslist = []

for i in np.arange(len(listings2nume['descriptions'])):
  words = desc[i]
  scoresl = [scores_dict.get(i) for i in words]
  sc = [x for x in scoresl if x is not None]
  prod = np.nanmean(sc)
  scoreslist.append(prod)

len(scoreslist) == len(listings2nume['descriptions'])

listings2nume['TF_IDF_Weight'] = scoreslist

listings2nume.head()

"""## **4. Sentiment Analysis**

**Performed sentiment analysis on customer comments to get an understanding of how positive or negative users experiences were with their stay at the different AirBnB's.**
"""

listing_comments

# import Beautiful Soup, NumPy and Pandas, etc
import bs4 as bs
import numpy as np
import pandas as pd
import re
import hashlib
 
# download NLTK classifiers - these are cached locally on your machine
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

# import ml classifiers
from nltk.tokenize import sent_tokenize # tokenizes sentences
from nltk.stem import PorterStemmer     # parsing/stemmer
from nltk.tag import pos_tag            # parts-of-speech tagging
from nltk.corpus import wordnet         # sentiment scores
from nltk.stem import WordNetLemmatizer # stem and context
from nltk.corpus import stopwords       # stopwords
from nltk.util import ngrams            # ngram iterator

# import word2vec
from gensim.test.utils import datapath
from gensim import utils
from gensim.models import Word2Vec

# import sklearn
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize, FunctionTransformer
from sklearn.feature_extraction.text import CountVectorizer

#to lowercase
listing_comments['comments'] = listing_comments['comments'].apply(lambda x: " ".join(x.lower() for x in x.split()))
listing_comments['comments']

#strip punctuation
listing_comments['comments'] = listing_comments['comments'].str.replace('[^\w\s]','')
listing_comments['comments']

#stopwords
stop = stopwords.words('english')
listing_comments['comments'] = listing_comments['comments'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
listing_comments['comments']

#stemming
st = PorterStemmer()
listing_comments['comments'] = listing_comments['comments'].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))
listing_comments['comments']

#sentiment scores for all comments

#first score is sentiment polarity (whether the sentiment is positive or negative 
#second score is subjectivity score (how subjective is the text)

from textblob import TextBlob
def senti(x):
    return TextBlob(x).sentiment  

listing_comments['senti_score'] = listing_comments['comments'].apply(senti)

listing_comments.senti_score.head()

#final table w/ senti_score
#range of polarity: -1 to 1(negative to positive)-- tells if text contains positive or negative feedback
listings_joined_dropped['sentiment'] = listing_comments.senti_score.apply(lambda x: x[0])
listings_joined_dropped.head()

listings_joined_dropped['tfidf_score'] = listings2nume['TF_IDF_Weight']
listings_joined_dropped.head()

listings_joined_dropped = listings_joined_dropped.drop(columns=['id'])

listings_joined_dropped.fillna(np.mean(listings_joined_dropped['tfidf_score']), inplace=True)

from sklearn import preprocessing

normalized_listings = listings_joined_dropped.values
min_max_scaler = preprocessing.MinMaxScaler()
normalized_listings = min_max_scaler.fit_transform(normalized_listings)
listings_final = pd.DataFrame(normalized_listings)

listings_final

"""## **5. KMeans Clustering**

**Finally, clustering! Grouping up our data to see what kind of similarities our algorithm can come up with!**




"""

#kmeans 
kvalues = np.arange(2, 20)
accuracy = []
clustervariance = {}
for k in kvalues:
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(listings_final)
  score = silhouette_score(listings_final, kmeans.labels_)
  accuracy.append(score)
  clustervariance[k] = kmeans.inertia_
#plotting elbow chart
plt.plot(kvalues, list(clustervariance.values()), 'bx-')
plt.xlabel('k')
plt.ylabel('SSE')
plt.title('The Elbow Method showing the optimal k')
plt.show()

def cluster_queen(df,k):
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(df) 
  cluster_labels = kmeans.labels_

  # assign these clusters back into the dataframe
  df = df.assign(cluster=cluster_labels)
  return df

listings_joined_dropped.head()

listings_joined_dropped = cluster_queen(listings_joined_dropped, 5)
listings_joined_dropped.head()

listings_joined_dropped.groupby('cluster').mean()

lat = listings.latitude
lon = listings.longitude
neighbourhood_cleansed = listings.neighbourhood_cleansed
name = listings.name
space = listings.property_type

"""## **6. User Interface**
**Visualizing our clusters on a map of San Francisco. These colour-coded visuals can directly be used by people looking to stay in AirBnB's to find the best fit for them! We employed recursive clustering to allow for a more narrow search!**
"""

!pip install geopandas
!pip install folium

import geopandas as gpd
import folium
from shapely.geometry import Point, Polygon
crs = {'init': 'epsg:4326'}

def mapping(df, lon, lat, name, neighbourhood_cleansed, space):
  df['latitude'] = lat
  df['longitude'] = lon
  geo_df = gpd.GeoDataFrame(df, crs=crs, geometry=gpd.points_from_xy(df.longitude, df.latitude))
  geo_df['neighborhood'] = neighbourhood_cleansed
  geo_df['name'] = name
  geo_df['space'] = space

  m1 = folium.Map(location=[37.79, -122.41], zoom_start=14.5)
  folium.GeoJson(geo_df.geometry).add_to(m1)

  for row in geo_df.iterrows():
    row_values = row[1]
    center_point = row_values['geometry']
    location = [center_point.y, center_point.x]
    if row_values['cluster'] == 0:
      marker_color = 'lightred'
    elif row_values['cluster'] == 1:
      marker_color = 'blue'
    elif row_values['cluster'] == 2:
      marker_color = 'lightgreen'
    elif row_values['cluster'] == 3:
      marker_color = 'pink'
    else:
      marker_color = 'lightblue'
    popup = ('Name: ' + str(row_values['name']) + '\n' + 'Price: $' + str(row_values['price']) + '\n' + 'Rating: ' + str(row_values['review_scores_rating'])
    + '\n' + 'Bedrooms: ' + str(row_values['bedrooms']) + '\n' + 'Type: ' + str(row_values['space']) + '\n' + 'Neighborhood: ' + str(row_values['neighborhood']))
    marker = folium.Marker(location = location, popup = popup, icon=folium.Icon(color=marker_color, icon='home'))
    marker.add_to(m1)

  display(m1)
  return geo_df

new_df = mapping(listings_joined_dropped,lon,lat,name,neighbourhood_cleansed, space)

cluster_dict = {"lightred" :0,"blue":1,"lightgreen":2,"pink":3,"lightblue":4}

green = new_df[new_df['cluster'] == cluster_dict['lightgreen']]
red = new_df[new_df['cluster'] == cluster_dict['lightred']]
blue = new_df[new_df['cluster'] == cluster_dict['blue']]
print(len(green),len(red),len(blue))

"""*Which cluster do you want to explore? Choose a color.*

"""

color = "blue"

df_blue = new_df[new_df['cluster'] == cluster_dict[color]]
name = df_blue["name"]
neighbourhood_cleansed = df_blue["neighborhood"]
lon = df_blue["longitude"]
lat = df_blue["latitude"]
space = df_blue["space"]
df_blue = df_blue.drop(columns= ["longitude","latitude","neighborhood","name","cluster","geometry", "space"])
df_blue.head()

listings_joined_dropped_blue = cluster_queen(df_blue, 5)

df_blue_map = mapping(listings_joined_dropped_blue, lon, lat, name, neighbourhood_cleansed, space)

"""*Which cluster do you want to explore next? Choose a color.*"""

color = "pink"

df_pink = new_df[new_df['cluster'] == cluster_dict[color]]
name = df_pink["name"]
neighbourhood_cleansed = df_pink["neighborhood"]
lon = df_pink["longitude"]
lat = df_pink["latitude"]
space = df_pink["space"]
df_pink = df_pink.drop(columns= ["longitude","latitude","neighborhood","name","cluster","geometry", "space"])
df_pink.head()

listings_joined_dropped_pink = cluster_queen(df_pink, 3)

df_pink_map = mapping(listings_joined_dropped_pink, lon, lat, name, neighbourhood_cleansed, space)